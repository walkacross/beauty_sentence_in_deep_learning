# beauty_sentence_in_deep_learning
beauty_sentence_in_deep_learning

## attention is all you need
Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1 , ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym ) of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.
